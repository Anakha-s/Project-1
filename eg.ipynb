{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eg.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anakha-s/Project-1/blob/main/eg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWvNjn-pJ4Ts"
      },
      "outputs": [],
      "source": [
        "## EDA Pkgs\n",
        "import pandas as pd\n",
        "#Data Viz Pkg\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "import numpy as np # linear algebra\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"covid19_tweets.csv\")\n",
        "# Preview\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "8NvN6kbTKAxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neattext\n",
        "# Load Text Cleaning Package\n",
        "import neattext.functions as nfx"
      ],
      "metadata": {
        "id": "vvR8I2uDKAy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Text\n",
        "df['clean_tweet'] = df['text'].apply(nfx.remove_hashtags)\n",
        "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: nfx.remove_userhandles(x))\n",
        "# Cleaning Text: Multiple WhiteSpaces\n",
        "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_multiple_spaces)\n",
        "# Cleaning Text : Remove urls\n",
        "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_urls)\n",
        "# Cleaning Text: Punctuations\n",
        "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_puncts)\n",
        "#Remove blank rows if any.\n",
        "df['clean_tweet'] .dropna(inplace=True)\n",
        "# Change all the text to lower case.\n",
        "df['clean_tweet']  = [entry.lower() for entry in df['clean_tweet'] ]\n",
        "df[['text','clean_tweet']]"
      ],
      "metadata": {
        "id": "duT9A0NOKmC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "df['clean_tweet']= [word_tokenize(entry) for entry in df['clean_tweet']]\n",
        "df['clean_tweet']\n",
        "\n"
      ],
      "metadata": {
        "id": "KNMCivDH5d7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_tweet'].head(1)"
      ],
      "metadata": {
        "id": "BlV4NnYdcOjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(words):\n",
        "  return [word for word in words if  not word in stop_words]\n",
        "df['clean_tweet']=df['clean_tweet'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "\n",
        "def stemming(words):\n",
        "  ps=PorterStemmer()\n",
        "  return [ps.stem(word) for word in words]\n",
        "df['clean_tweet']=df['clean_tweet'].apply(lambda x: stemming(x))\n",
        "\n",
        "def lemmatizing(words):\n",
        "    lemmatizer =WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "df['clean_tweet']=df['clean_tweet'].apply(lambda x: lemmatizing(x))\n",
        "\n",
        "def final_text(words):\n",
        "     return ' '.join(words)\n",
        "df['clean_tweet']=df['clean_tweet'].apply(lambda x:final_text(x))\n",
        "   \n"
      ],
      "metadata": {
        "id": "TUvV8uIr5joa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_tweet'].head(1)"
      ],
      "metadata": {
        "id": "dx2Wt1tjcqZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment_polarity = blob.sentiment.polarity\n",
        "    sentiment_subjectivity = blob.sentiment.subjectivity\n",
        "    if sentiment_polarity > 0:\n",
        "        sentiment_label = 1\n",
        "    elif sentiment_polarity < 0:\n",
        "        sentiment_label = -1\n",
        "    else:\n",
        "        sentiment_label = 0\n",
        "    result = {'polarity':sentiment_polarity,\n",
        "              'subjectivity':sentiment_subjectivity,\n",
        "              'sentiment':sentiment_label}\n",
        "    return result"
      ],
      "metadata": {
        "id": "Yi_6tn-q118y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment_results'] = df['clean_tweet'].apply(get_sentiment)\n",
        "df['sentiment_results']"
      ],
      "metadata": {
        "id": "XjKCGnVAi-57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.join(pd.json_normalize(df['sentiment_results']))\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "j--51Y5MpXN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df['clean_tweet'],df['sentiment'],test_size=0.3)"
      ],
      "metadata": {
        "id": "qF4b0ZI-dklg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(Train_Y)\n",
        "Test_Y = Encoder.fit_transform(Test_Y)"
      ],
      "metadata": {
        "id": "zRKTktgEudFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tfidf_vect = TfidfVectorizer(max_features=5)\n",
        "Tfidf_vect.fit(df['clean_tweet'])\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
        "\n",
        "print(Tfidf_vect.vocabulary_)"
      ],
      "metadata": {
        "id": "tBPBbhSAudT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Train_X_Tfidf)"
      ],
      "metadata": {
        "id": "HM8sFzJIudc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier - Algorithm - SVM\n",
        "# fit the training dataset on the classifier\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
      ],
      "metadata": {
        "id": "VZJff9gefCcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(Test_Y,predictions_SVM)"
      ],
      "metadata": {
        "id": "IJQWO0-7iB1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Test_Y,predictions_SVM))"
      ],
      "metadata": {
        "id": "yXgnsWb8ioVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVCmodel = LinearSVC()\n",
        "SVCmodel.fit(X_train, y_train)\n",
        "model_Evaluate(SVCmodel)\n",
        "y_pred2 = SVCmodel.predict(X_test)"
      ],
      "metadata": {
        "id": "9qG1-yVFgBPp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}